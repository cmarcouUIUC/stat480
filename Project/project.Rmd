---
title: "480 Project"
author: "Charlie Marcou, Carrie Mecca"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
drugsComTrain_raw <- read_delim("drugsComTrain_raw.tsv", delim = "\t", escape_double = FALSE,  trim_ws = TRUE)
```

```{r}
library(tm)
dtm <- DocumentTermMatrix(drugsComTrain_raw$review,
                          control = list(stopwords = TRUE, 
                                         removeNumbers = TRUE,
                                         removePunctuation = TRUE,
                                         stemming = TRUE))
dtm <- removeSparseTerms(dtm,0.995) #remove infrequent terms
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ]           #remove all docs without words

```

```{r}
#look at matrix
head(dtm.new)
inspect(dtm.new[1:5, 1000:1005])
```

```{r}
#topic modeling
library(maptpx)
x <- as.simple_triplet_matrix(dtm.new)

tpcs <- topics(x,K=c(5,10), verb=10) #try multiple Ks
dim(tpc$theta)
colSums(tpc$theta)

summary(tpcs, n=10) #prints top 10 words per document
```

```{r}
library(wordcloud)
color_palette <- colors=brewer.pal(8, "Dark2") #however many need based on k picked

for (i in 1:10) {
wordcloud(row.names(tpcs$theta), 
	freq=tpcs$theta[,i], min.freq=0.004, col=color_palette[i])
}
```

```{r}
##regress words vs rating
x.words <- 100*dtm.new/rowSums(dtm.new)
regwords.cv <- cv.gamlr(x.words, drugsComTrain_raw$rating)
##regress words vs useful count
regwords.cv.2 <- cv.gamlr(x.words, drugsComTrain_raw$usefulCount)
```

step 1 is to probably create word matrix

ok some ideas
--build topic model and or cluster words
--predict rating / review usefulness from words/bigrams/phrases
--if we do do a prediction compare straight glm/fdr analysis with lasso
--perhaps do a double lasso controlling for other variables like date/condition/review/usefulness.

