---
title: "Homework 2 Assignment"
author: "Carrie Mecca, Charlie Marcou, Jessie Bustin and Jasmine Zhang"
fontsize: 10 pt
output: 
    pdf_document:
        fig_width: 6
        fig_height: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      #include = TRUE, 
                      fig.width = 6, fig.height = 4,
                      results = FALSE,
                      warning = FALSE,
                      cache = FALSE,
                      digits = 3,
                      width = 48) 
```

**This assignment contains TWO data analyses.** Each question is 10 points.

# Predicting House Prices

```{r}
## Read in the data

homes <- read.csv("homes2004.csv")

# conditional vs marginal value


par(mfrow=c(1,2)) # 1 row, 2 columns of plots 

hist(homes$VALUE, col="grey", xlab="home value", main="")

plot(VALUE ~ factor(BATHS), 
    col=rainbow(8), data=homes[homes$BATHS<8,],
    xlab="number of bathrooms", ylab="home value")
```

```{r}
# You can try some quick plots.  Do more to build your intuition!

#par(mfrow=c(1,2)) 
#plot(VALUE ~ STATE, data=homes, 
#	col=rainbow(nlevels(homes$STATE)), 
#	ylim=c(0,10^6), cex.axis=.65)
#plot(gt20dwn ~ FRSTHO, data=homes, 
#	col=c(1,3), xlab="Buyer's First Home?", 
#	ylab="Greater than 20% down")
```


## Question 1

Regress log price onto all variables but mortgage. What is the R2? How many coefficients are used in this model and how many are significant at 10% FDR? Re-run regression with only the significant covariates, and compare R2 to the full model.

The R^2 is .447301. There are 40 coefficients. Of these 40 coefficients, we can see that 5 are not significant. The five that are no significant are: "ETRANSY" "STATECO" "STATECT" "BEDRMS" and "NUNITS"
```{r}
library(knitr) # library for nice R markdown output
# regress log(PRICE) on everything except AMMORT 

pricey <- glm(log(LPRICE) ~ .-AMMORT, data=homes)
cor(pricey$fitted,log(homes$LPRICE))^2
summary(pricey)
# extract pvalues

pvals <- summary(pricey)$coef[-1,4]

# example: those variable insignificant at alpha=0.05

names(pvals)[pvals>.10]
```
We drop BEDRMS and NUNITS, but do not drop the insignificant categorial levels. After this, the R^2 of the reduced model is 0.4471981. In comparison to the full model, there is a slight decrease in the R^2. ###Not sure if its helpful or neccessary to add the following: The shrinkage between the R^2 of the two models is .0001029.
```{r}
#fit regression without non-significant variables
pricey2 <- glm(log(LPRICE) ~ .-AMMORT -BEDRMS -NUNITS, data=homes) ##drop BEDRMS, NUNITS. leave ETRANSY, STATECO and STATECT since categorical levels.
summary(pricey2)
cor(pricey2$fitted,log(homes$LPRICE))^2

# you'll want to replace .05 with your FDR cutoff
# you can use the `-AMMORT' type syntax to drop variables
```

## Question 2

Fit a regression for whether the buyer had more than 20 percent down (onto everything but AMMORT and LPRICE). Interpret effects for Pennsylvania state, 1st home buyers and the number of bathrooms. Add and describe an interaction between 1st home-buyers and the number of baths.


For a buyer in pennsylvania we expect the log odds of the downpayment being greater than 20% to increase by .6011
For a one unit increase in bedrooms, we expect the log odds of the downpayment being greater than 20% to change by -.00286
For a first time home buyer, we expect the log odds of the the downpayment being greater than 20% to change by -.37

After expanding the model, we see a negative interaction between 1st time home buyers and number of bathrooms. This indicates that the log odds of the downpayment greater than 20% for each additional bathroom, will be lower for a first time home buyer than a buyer who has bough a home previously. 
```{r}
# create a var for downpayment being greater than 20%
homes$gt20dwn <- 
  factor(0.2<(homes$LPRICE-homes$AMMORT)/homes$LPRICE)

pricey3 <- glm(homes$gt20dwn ~ .-AMMORT -LPRICE ,data=homes, family=binomial)
summary(pricey3)
##Should we add and describe an interaction in this model or in a new model?

pricey4 <- glm(homes$gt20dwn  ~ .-AMMORT -LPRICE + FRSTHO:BATHS ,data=homes, family=binomial)
summary(pricey4)

# - don't forget family="binomial"!
# - use +A*B in forumula to add A interacting with B
```


## Question 3

Focus only on a subset of homes worth > 100k. Train the full model from Question 1 on this subset. Predict the left-out homes using this model. What is the out-of-sample fit (i.e. $R^2$)? Explain why you get this value.

For the training data, the subset of homes worth > 100k, the R^2 is 0.3919407. The out-of-sample data, however, has a lower R^2 at only 0.1834929. The reason the test set R^2 is lower is because the model is overfitting on the subset of higher-priced homes and, subsequently, is unable to extrapolate to and perform as well on the lower-priced subset. #may rewrite #I think we can probably say "and, subsequently, is unable to perform as well on the lower-priced subset. Using the model to extrapolate to homes with smaller prices would not be reasonable." something like this?? - charlie

```{r}
subset <- which(homes$VALUE>100000)
# Use the code ``deviance.R" to compute OOS deviance

source("deviance.R")

# Null model has just one mean parameter

ybar <- mean(log(homes$LPRICE[-subset]))

D0 <- deviance(y=log(homes$LPRICE[-subset]), pred=ybar)

subset_pricey <- glm(log(LPRICE) ~ .-AMMORT, data=homes[subset,])
cor(subset_pricey$fitted,log(homes[subset,c("LPRICE")]))^2
summary(subset_pricey)

predictions <- predict(subset_pricey, homes[-subset,])
cor(predictions,log(homes[-subset,c("LPRICE")]))^2 
```

# Amazon Reviews

We will use the same datasets (review_subset.csv, word_freq.csv and words.csv) as in Assignment 1.

```{r}
data<-read.table("Review_subset.csv",header=TRUE)

words<-read.table("words.csv")
words<-words[,1]

doc_word<-read.table("word_freq.csv")
names(doc_word)<-c("Review ID","Word ID","Times Word" )
```

## Question 4

We want to build a predictor of customer ratings from product reviews and product attributes. For these questions, you will fit a LASSO path of logistic regression using a binary outcome: 
\begin{align}
Y=1& \quad\text{for  5 stars}\\
Y=0& \quad \text{for less than 5 stars}.
\end{align}


Fit a LASSO model with only product categories. The start code prepares a sparse design matrix of 142 product categories. What is the in-sample R2 for the AICc slice of the LASSO path? Why did we use standardize FALSE?

We can use standardize false because the x's are on the same scale already because they are all factor levels. not sure what is meant by r2 for aicc slice of lasso path. The in-sample R2 for the best lambda is 0.1048737.

```{r data, results='asis'}

# Let's define the binary outcome

# Y=1 if the rating was 5 stars

# Y=0 otherwise

Y<-as.numeric(data$Score==5)

# (a) Use only product category as a predictor

library(gamlr)

source("naref.R") 

# Cast the product category as a factor
data$Prod_Category<-as.factor(data$Prod_Category)

class(data$Prod_Category)


# Since product category is a factor, we want to relevel it for the LASSO. 
# We want each coefficient to be an intercept for each factor level rather than a contrast. 
# Check the extra slides at the end of the lecture.
# look inside naref.R. This function relevels the factors for us.

data$Prod_Category<-naref(data$Prod_Category)

# Create a design matrix using only products

products<-data.frame(data$Prod_Category)

x_cat<-sparse.model.matrix(~., data=products)[,-1]

# Sparse matrix, storing 0's as .'s 
# Remember that we removed intercept so that each category 
# is standalone, not a contrast relative to the baseline category

colnames(x_cat)<-levels(data$Prod_Category)[-1]

# let's call the columns of the sparse design matrix as the product categories

# Let's fit the LASSO with just the product categories

lasso1<- gamlr(x_cat, 	y=Y, standardize=FALSE,family="binomial",
lambda.min.ratio=1e-3)


plot(lasso1)

lasso1.summary <- summary(lasso1)
lasso1.summary[ (lasso1.summary$aicc ==min(lasso1.summary$aicc)),] #selected the row with the min aic. idk if that's what she meant ##I think that makes sense? I couldn't really tell what she was asking at all but I think this is a good best guess.
```


## Question 5

Fit a LASSO model with both product categories and the review content (i.e. the frequency of occurrence of words). Use AICc to select lambda.
How many words were selected as predictive of a  5 star review? Which 10 words have the most positive effect on odds of a 5 star review? What is the interpretation of the coefficient for the word `discount'? 

At a log-lambda of -8.334091, we select 1154 words and categories

The top 10 words with positive effect on odds of a 5 star review are: worried, Breads, plus, Almond Leaveners & Yeasts, excellently, find, grains, Computers, Features, hound. ###this is weird, i think she probably wants just the words not words and categories. Not sure how to get the top ten of those programatically though. ##i don't get how we would separate them either but why include categories if she was just interested in the words? - Carrie

The coefficient of discount is: 6.961539. This means that we expect log odds to increase by 6.961539 when the word discount is included in a review. 
```{r xtable, results='asis'}

# Fit a LASSO with all 142 product categories and 1125 words 

spm<-sparseMatrix(i=doc_word[,1],
                  j=doc_word[,2],
                  x=doc_word[,3],
                  dimnames=list(id=1:nrow(data),
                  words=words))

dim(spm) # 13319 reviews using 1125 words

x_cat2<-cbind(x_cat,spm)

lasso2 <- gamlr(x_cat2, y=Y,lambda.min.ratio=1e-3,family="binomial")

plot(lasso2)

lassobeta <- coef(lasso2) 

#get log lambda
log(lasso2$lambda[which.min(AICc(lasso2))])

sum(lassobeta!=0) ##1154 selected? might need to figure out how to subset this to just words?

coefmat=as.matrix(coef(lasso2))

head(coefmat[order(coefmat[, 1], decreasing=TRUE),],10)

coefmat['discount',]
```


## Question 6

Continue with the model from Question 5.
Run cross-validation to obtain the best lambda value that minimizes OOS deviance. How many coefficients are nonzero then? How many are nonzero under the 1se rule?  

The lambda value that minimizes OOS deviance is 0.00137444. With that lambda, there are 974 non-zero coefficients. Using the 1se rule instead, there are only 831 non-zero coefficients (with a random seed of 250).


```{r xtable data, results='asis'}

cv.fit <- cv.gamlr(x_cat2,
				   y=Y,
				   lambda.min.ratio=1e-3,
				   family="binomial",
				   verb=TRUE)

min_lambda <- cv.fit$lambda.min
min_lambda

cv.fit.coef <- coef(cv.fit, select="min") #coeff 
length(rownames(cv.fit.coef)[cv.fit.coef[,1]!= 0])

cv.fit.coef2 <- coef(cv.fit, select="1se") #coeff 
length(rownames(cv.fit.coef2)[cv.fit.coef2[,1]!= 0])
```


